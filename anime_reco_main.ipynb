{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornac.models import ItemKNN, UserKNN, SVD, WMF\n",
    "from cornac.eval_methods.base_method import BaseMethod\n",
    "from cornac.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from load_filter_and_sample import load_and_filter_data, sample_users\n",
    "import cornac.metrics as met\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 2840\n",
    "anime_id = 5 \n",
    "\n",
    "k_values = [5, 10, 15, 30, 50, 100]\n",
    "threshold = 3.5\n",
    "top_n = 10\n",
    "\n",
    "split_percentage = 0.8      # 80% of the data will be used for training and 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_matrix(similarity_matrix, similarity_metric, base):\n",
    "    plt.figure()\n",
    "    plt.imshow(similarity_matrix, cmap='hot', interpolation='nearest')\n",
    "    plt.title(f\"{base}-{base} {similarity_metric} Similarity Matrix\")\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORT_PATH_BASE = \"datasets/\"\n",
    "user_path = IMPORT_PATH_BASE + \"user-filtered.csv\"\n",
    "item_path = IMPORT_PATH_BASE + \"anime-dataset-2023.csv\"\n",
    "\n",
    "user_df, anime_df = load_and_filter_data(user_path, item_path, filtering=False, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.drop(columns=['Other name', 'Name', 'Synopsis', 'Source', 'Premiered', 'Status', 'Producers', 'Licensors', 'Duration'], inplace=True) # Drop unnecessary columns\n",
    "anime_df.rename(columns={'English name': 'Name'}, inplace=True) # Rename 'English name' to 'Name'\n",
    "anime_df = anime_df.drop(anime_df[anime_df.eq('UNKNOWN').any(axis=1)].index) # Drop rows with 'UNKNOWN' values\n",
    "anime_df = anime_df[anime_df['Type'].isin(['Movie', 'TV', 'TV Short'])] # Only keep Movies, TV and TV Short\n",
    "anime_df = anime_df[anime_df['anime_id'].isin(user_df['anime_id'])] # Only keep items that are in user_df\n",
    "anime_df['Score'] = anime_df['Score'].astype(float)\n",
    "anime_df['Episodes'] = anime_df['Episodes'].astype(float)\n",
    "anime_df['Members'] = anime_df['Members'].astype(float)\n",
    "anime_df['Favorites'] = anime_df['Favorites'].astype(float)\n",
    "anime_df['Popularity'] = anime_df['Popularity'].astype(float)\n",
    "anime_df['Rank'] = anime_df['Rank'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df[user_df['anime_id'].isin(anime_df['anime_id'])] # Only keep users that are in item_df\n",
    "user_df = user_df[user_df['rating'] > 0] # Remove reviews with rating 0 because it is not a valid rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_lot_reviews = sample_users(user_df, min_reviews=400, max_reviews=600, n_users=150)\n",
    "users_medium_reviews = sample_users(user_df, min_reviews=100, max_reviews=200, n_users=400)\n",
    "users_small_reviews = sample_users(user_df, min_reviews=50, max_reviews=100, n_users=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.concat([users_lot_reviews, users_medium_reviews, users_small_reviews]).reset_index(drop=True)\n",
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = anime_df['anime_id'].values\n",
    "users = user_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_percentage):\n",
    "    def split_group(group):\n",
    "        group = group.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "        split_index = int(len(group) * split_percentage)\n",
    "        return group.iloc[:split_index], group.iloc[split_index:]\n",
    "    \n",
    "    grouped = data.groupby('user_id')\n",
    "    \n",
    "    train_list, test_list = zip(*grouped.apply(lambda x: split_group(x)).values)\n",
    "    \n",
    "    train_set = pd.concat(train_list).reset_index(drop=True)\n",
    "    test_set = pd.concat(test_list).reset_index(drop=True)\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_data(user_df, split_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_uir(train[['user_id', 'anime_id', 'rating']].values.tolist(), seed=42)\n",
    "test_dataset = Dataset.from_uir(test[['user_id', 'anime_id', 'rating']].values.tolist(), seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_method = BaseMethod.from_splits(train[['user_id', 'anime_id', 'rating']].values, test[['user_id', 'anime_id', 'rating']].values)\n",
    "metrics = [met.RMSE(), met.Precision(k=10), met.Recall(k=10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemcf = ItemKNN(k=10, similarity=\"cosine\", verbose=True)\n",
    "itemcf.fit(train_dataset)\n",
    "result = eval_method.evaluate(itemcf, metrics, user_based=False)\n",
    "for metric in result:\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usercf = UserKNN(k=10, similarity=\"cosine\", verbose=True)\n",
    "usercf.fit(train_dataset)\n",
    "result = eval_method.evaluate(usercf, metrics, user_based=False)\n",
    "for metric in result:\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Factorization using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdcf=SVD()\n",
    "svdcf.fit(train_dataset)\n",
    "result = eval_method.evaluate(svdcf, metrics, user_based=False)\n",
    "for metric in result:\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Factorization using WMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmfcf = WMF()\n",
    "wmfcf.fit(train_dataset)\n",
    "result = eval_method.evaluate(wmfcf, metrics, user_based=False)\n",
    "print(\"WMF\")\n",
    "for metric in result:\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_id in test_dataset.uid_map:\n",
    "    user_idx = test_dataset.uid_map[user_id]\n",
    "else:\n",
    "    print(f\"user_id {user_id} not found in test_dataset. Using a random user.\")\n",
    "    user_id = np.random.choice(test_dataset.user_ids)\n",
    "    user_idx = test_dataset.uid_map[user_id]\n",
    "    print(f\"Random user_id: {user_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(model, user_idx, top_n=10):\n",
    "    scores = model.score(user_idx)\n",
    "    ranked_indices = np.argsort(-scores)\n",
    "    return ranked_indices[:top_n]\n",
    "\n",
    "user_idx = test_dataset.uid_map[user_id]\n",
    "\n",
    "itemcf_recos = get_recommendations(itemcf, user_idx)\n",
    "print(itemcf_recos)\n",
    "usercf_recos = get_recommendations(usercf, user_idx)\n",
    "print(usercf_recos)\n",
    "svd_recos = get_recommendations(svdcf, user_idx)\n",
    "print(svd_recos)\n",
    "wmf_recos = get_recommendations(wmfcf, user_idx)\n",
    "print(wmf_recos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_anime_ids(indices, dataset):\n",
    "    return [dataset.iid_map[idx] for idx in indices if idx in dataset.iid_map]\n",
    "\n",
    "itemcf_anime_ids = indices_to_anime_ids(itemcf_recos, test_dataset)\n",
    "usercf_anime_ids = indices_to_anime_ids(usercf_recos, test_dataset)\n",
    "svd_anime_ids = indices_to_anime_ids(svd_recos, test_dataset)\n",
    "wmf_anime_ids = indices_to_anime_ids(wmf_recos, test_dataset)\n",
    "\n",
    "print(\"ItemKNN Recommendations:\")\n",
    "print(itemcf_anime_ids)\n",
    "print()\n",
    "print(\"UserKNN Recommendations:\")\n",
    "print(usercf_anime_ids)\n",
    "print()\n",
    "print(\"SVD Recommendations:\")\n",
    "print(svd_anime_ids)\n",
    "print()\n",
    "print(\"WMF Recommendations:\")\n",
    "print(wmf_anime_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversifying Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = itemcf.sim_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise :\n",
    "- Maximal Marginal Relevance (MMR)\n",
    "- Max-Sum Diversification (MSD)\n",
    "- Diversity-weighted Utility Maximization (DUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divers_recos(method, relevance_scores, similarity_matrix, alpha=0.5, top_n=10):\n",
    "    selected_items = []\n",
    "    item_indices = list(range(len(relevance_scores)))\n",
    "    \n",
    "    while len(selected_items) < top_n and item_indices:\n",
    "        if not selected_items:\n",
    "            next_item = item_indices.pop(np.argmax(relevance_scores[item_indices]))\n",
    "        else:\n",
    "            max_score = -np.inf\n",
    "            next_item = None\n",
    "            for item in item_indices:\n",
    "                diversity = sum([1 - similarity_matrix[item, selected_item] for selected_item in selected_items])\n",
    "                if method == \"mmr\":\n",
    "                    score = alpha * relevance_scores[item] - (1 - alpha) * (diversity / len(selected_items))\n",
    "                elif method == \"msd\":\n",
    "                    score = relevance_scores[item] + diversity\n",
    "                elif method == \"dum\":\n",
    "                    score = alpha * relevance_scores[item] + (1 - alpha) * diversity\n",
    "                \n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    next_item = item\n",
    "            item_indices.remove(next_item)\n",
    "        selected_items.append(next_item)\n",
    "    \n",
    "    return selected_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_scores = np.array([itemcf.score(user_idx, i) for i in range(test_dataset.num_items)])\n",
    "\n",
    "mmr_recos = divers_recos(\"mmr\", relevance_scores, similarity_matrix, alpha=0.5, top_n=top_n)\n",
    "msd_recos = divers_recos(\"msd\", relevance_scores, similarity_matrix, alpha=0.5, top_n=top_n)\n",
    "dum_recos = divers_recos(\"dum\", relevance_scores, similarity_matrix, alpha=0.5, top_n=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_anime(anime_id):\n",
    "    anime = anime_df[anime_df['anime_id'] == anime_id]\n",
    "\n",
    "    try:\n",
    "        url = anime['Image URL'].values[0]\n",
    "        title = anime['Name'].values[0]\n",
    "\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    except:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MMR Recommendations:\")\n",
    "for idx in mmr_recos:\n",
    "    if idx in test_dataset.iid_map:\n",
    "        anime_id = test_dataset.iid_map[idx]\n",
    "        print(f\"Anime ID: {anime_id}, Rating: {relevance_scores[idx]:.2f}\")\n",
    "print()\n",
    "print(\"MSD Recommendations:\")\n",
    "for idx in msd_recos:\n",
    "    if idx in test_dataset.iid_map:\n",
    "        anime_id = test_dataset.iid_map[idx]\n",
    "        print(f\"Anime ID: {anime_id}, Rating: {relevance_scores[idx]:.2f}\")\n",
    "print()\n",
    "print(\"DUM Recommendations:\")\n",
    "for idx in dum_recos:\n",
    "    if idx in test_dataset.iid_map:\n",
    "        anime_id = test_dataset.iid_map[idx]\n",
    "        print(f\"Anime ID: {anime_id}, Rating: {relevance_scores[idx]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_anime_list(anime_ids):\n",
    "    for anime_id in anime_ids:\n",
    "        display_anime(anime_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_anime_ids = indices_to_anime_ids(mmr_recos, test_dataset)\n",
    "msd_anime_ids = indices_to_anime_ids(msd_recos, test_dataset)\n",
    "dum_anime_ids = indices_to_anime_ids(dum_recos, test_dataset)\n",
    "\n",
    "print(\"ItemKNN Recommendations:\")\n",
    "display_anime_list(itemcf_anime_ids)\n",
    "print()\n",
    "print(\"UserKNN Recommendations:\")\n",
    "display_anime_list(usercf_anime_ids)\n",
    "print()\n",
    "print(\"SVD Recommendations:\")\n",
    "display_anime_list(svd_anime_ids)\n",
    "print()\n",
    "print(\"WMF Recommendations:\")\n",
    "display_anime_list(wmf_anime_ids)\n",
    "print()\n",
    "print(\"MMR Recommendations:\")\n",
    "display_anime_list(mmr_anime_ids)\n",
    "print()\n",
    "print(\"MSD Recommendations:\")\n",
    "display_anime_list(msd_anime_ids)\n",
    "print()\n",
    "print(\"DUM Recommendations:\")\n",
    "display_anime_list(dum_anime_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
